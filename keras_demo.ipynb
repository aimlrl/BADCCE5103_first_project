{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle as kg\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KAGGLE_USERNAME\"] = \"aimlrl\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"54d4150a6ca782d7b27af3f3754eddd8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.api.dataset_download_files(dataset=\"medahmedkrichen/devanagari-handwritten-character-datase\",\n",
    "                              path=\"dataset\",unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_df(path):\n",
    "\n",
    "    img_path = list()\n",
    "    img_label = list()\n",
    "\n",
    "    for single_class_dir_path in pathlib.Path(path).glob(\"*\"):\n",
    "\n",
    "        for single_class_img_path in pathlib.Path(single_class_dir_path).glob(\"*.png\"):\n",
    "\n",
    "            img_path.append(str(single_class_img_path))\n",
    "            #print(str(single_class_img_path).split(\"/\")[-2].split(\"_\")[-1])\n",
    "            img_label.append(str(single_class_img_path).split(\"/\")[-2].split(\"_\")[-1])\n",
    "\n",
    "    return pd.DataFrame(data={\"img_path\":img_path,\"label\":img_label})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"dataset/DevanagariHandwrittenCharacterDataset/Train\"\n",
    "test_path = \"dataset/DevanagariHandwrittenCharacterDataset/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = train_test_df(train_path)\n",
    "testing_data = train_test_df(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "character2int = dict(zip(training_data[\"label\"].unique(),range(len(training_data[\"label\"].unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1578/1986340858.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  training_data[\"label\"].replace(to_replace=character2int.keys(),value=character2int.values(),\n",
      "/tmp/ipykernel_1578/1986340858.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  training_data[\"label\"].replace(to_replace=character2int.keys(),value=character2int.values(),\n",
      "/tmp/ipykernel_1578/1986340858.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  testing_data.replace(to_replace=character2int.keys(),value=character2int.values(),\n"
     ]
    }
   ],
   "source": [
    "training_data[\"label\"].replace(to_replace=character2int.keys(),value=character2int.values(),\n",
    "                               inplace=True)\n",
    "\n",
    "testing_data.replace(to_replace=character2int.keys(),value=character2int.values(),\n",
    "                     inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true_train = to_categorical(y=training_data[\"label\"],num_classes=46)\n",
    "Y_true_test = to_categorical(y=testing_data[\"label\"],num_classes=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_dnn():\n",
    "\n",
    "    input_to_dnn = Input(shape=(1024,))\n",
    "    first_dense_out = Dense(units=1024,activation=\"relu\") (input_to_dnn)\n",
    "    output = Dense(units=46,activation=\"softmax\") (first_dense_out)\n",
    "\n",
    "    return Model(inputs=[input_to_dnn],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_generator(data_df, Y_true, mb_size):\n",
    "\n",
    "    for time_step in range(data_df.shape[0]//mb_size):\n",
    "        X_mb = list()\n",
    "\n",
    "        for img_path in data_df.iloc[time_step*mb_size:(time_step+1)*mb_size,0]:\n",
    "\n",
    "            img_np_array = plt.imread(img_path)\n",
    "            reshaped_np_array = img_np_array.reshape(1024,)\n",
    "            X_mb.append(reshaped_np_array)\n",
    "\n",
    "        X_mb = np.array(X_mb)\n",
    "        Y_true_mb = Y_true[time_step*mb_size:(time_step+1)*mb_size]\n",
    "\n",
    "        yield X_mb, Y_true_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "training_data_mb_size = 782\n",
    "testing_data_mb_size = 138\n",
    "training_data_generator = custom_data_generator(training_data,Y_true_train,training_data_mb_size)\n",
    "testing_data_generator = custom_data_generator(testing_data,Y_true_test,testing_data_mb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1024)]            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 46)                47150     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1096750 (4.18 MB)\n",
      "Trainable params: 1096750 (4.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = multiclass_dnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(Y_true_mb,Y_pred_mb):\n",
    "\n",
    "    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true=Y_true_mb,\n",
    "                                                                   y_pred=Y_pred_mb))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(X_true_train_mb,Y_true_train_mb):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "            \n",
    "        Y_pred_train_mb = model(X_train_mb, training=True)\n",
    "        training_loss = loss_fn(Y_true_train_mb, Y_pred_train_mb)\n",
    "\n",
    "    grads = tape.gradient(training_loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    train_acc_metric.update_state(Y_true_train_mb,Y_pred_train_mb)\n",
    "\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def testing_forward_pass(X_test_mb,Y_true_test_mb):\n",
    "\n",
    "    Y_pred_test_mb = model(X_test_mb,training=False)\n",
    "    testing_loss = loss_fn(Y_true_test_mb,Y_pred_test_mb)\n",
    "    test_acc_metric.update_state(Y_true_test_mb,Y_pred_test_mb)\n",
    "\n",
    "    return testing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Time Step 1, Training loss for one mini batch: 5.6247\n",
      "Epoch 1, Time Step 2, Training loss for one mini batch: 4.1371\n",
      "Epoch 1, Time Step 3, Training loss for one mini batch: 4.3814\n",
      "Epoch 1, Time Step 4, Training loss for one mini batch: 3.2353\n",
      "Epoch 1, Time Step 5, Training loss for one mini batch: 3.2523\n",
      "Epoch 1, Time Step 6, Training loss for one mini batch: 2.7486\n",
      "Epoch 1, Time Step 7, Training loss for one mini batch: 2.8350\n",
      "Epoch 1, Time Step 8, Training loss for one mini batch: 3.3826\n",
      "Epoch 1, Time Step 9, Training loss for one mini batch: 2.4872\n",
      "Epoch 1, Time Step 10, Training loss for one mini batch: 2.8634\n",
      "Epoch 1, Time Step 11, Training loss for one mini batch: 1.8471\n",
      "Epoch 1, Time Step 12, Training loss for one mini batch: 3.9690\n",
      "Epoch 1, Time Step 13, Training loss for one mini batch: 2.5065\n",
      "Epoch 1, Time Step 14, Training loss for one mini batch: 4.1761\n",
      "Epoch 1, Time Step 15, Training loss for one mini batch: 2.8203\n",
      "Epoch 1, Time Step 16, Training loss for one mini batch: 3.7198\n",
      "Epoch 1, Time Step 17, Training loss for one mini batch: 2.9221\n",
      "Epoch 1, Time Step 18, Training loss for one mini batch: 3.3183\n",
      "Epoch 1, Time Step 19, Training loss for one mini batch: 3.1962\n",
      "Epoch 1, Time Step 20, Training loss for one mini batch: 2.9319\n",
      "Epoch 1, Time Step 21, Training loss for one mini batch: 3.3758\n",
      "Epoch 1, Time Step 22, Training loss for one mini batch: 2.5417\n",
      "Epoch 1, Time Step 23, Training loss for one mini batch: 2.9912\n",
      "Epoch 1, Time Step 24, Training loss for one mini batch: 1.7943\n",
      "Epoch 1, Time Step 25, Training loss for one mini batch: 4.2868\n",
      "Epoch 1, Time Step 26, Training loss for one mini batch: 2.4071\n",
      "Epoch 1, Time Step 27, Training loss for one mini batch: 3.7445\n",
      "Epoch 1, Time Step 28, Training loss for one mini batch: 2.5881\n",
      "Epoch 1, Time Step 29, Training loss for one mini batch: 3.2475\n",
      "Epoch 1, Time Step 30, Training loss for one mini batch: 2.5168\n",
      "Epoch 1, Time Step 31, Training loss for one mini batch: 2.8953\n",
      "Epoch 1, Time Step 32, Training loss for one mini batch: 3.0282\n",
      "Epoch 1, Time Step 33, Training loss for one mini batch: 2.5191\n",
      "Epoch 1, Time Step 34, Training loss for one mini batch: 3.0550\n",
      "Epoch 1, Time Step 35, Training loss for one mini batch: 2.2710\n",
      "Epoch 1, Time Step 36, Training loss for one mini batch: 3.2308\n",
      "Epoch 1, Time Step 37, Training loss for one mini batch: 1.9802\n",
      "Epoch 1, Time Step 38, Training loss for one mini batch: 4.0095\n",
      "Epoch 1, Time Step 39, Training loss for one mini batch: 2.6230\n",
      "Epoch 1, Time Step 40, Training loss for one mini batch: 4.0069\n",
      "Epoch 1, Time Step 41, Training loss for one mini batch: 2.9984\n",
      "Epoch 1, Time Step 42, Training loss for one mini batch: 3.1106\n",
      "Epoch 1, Time Step 43, Training loss for one mini batch: 2.5816\n",
      "Epoch 1, Time Step 44, Training loss for one mini batch: 2.5044\n",
      "Epoch 1, Time Step 45, Training loss for one mini batch: 2.4872\n",
      "Epoch 1, Time Step 46, Training loss for one mini batch: 1.9677\n",
      "Epoch 1, Time Step 47, Training loss for one mini batch: 2.1915\n",
      "Epoch 1, Time Step 48, Training loss for one mini batch: 1.6888\n",
      "Epoch 1, Time Step 49, Training loss for one mini batch: 4.1818\n",
      "Epoch 1, Time Step 50, Training loss for one mini batch: 2.5306\n",
      "Epoch 1, Time Step 51, Training loss for one mini batch: 4.5764\n",
      "Epoch 1, Time Step 52, Training loss for one mini batch: 3.0636\n",
      "Epoch 1, Time Step 53, Training loss for one mini batch: 3.4766\n",
      "Epoch 1, Time Step 54, Training loss for one mini batch: 2.6124\n",
      "Epoch 1, Time Step 55, Training loss for one mini batch: 3.0786\n",
      "Epoch 1, Time Step 56, Training loss for one mini batch: 2.7602\n",
      "Epoch 1, Time Step 57, Training loss for one mini batch: 2.6168\n",
      "Epoch 1, Time Step 58, Training loss for one mini batch: 2.8440\n",
      "Epoch 1, Time Step 59, Training loss for one mini batch: 2.4066\n",
      "Epoch 1, Time Step 60, Training loss for one mini batch: 3.3333\n",
      "Epoch 1, Time Step 61, Training loss for one mini batch: 2.0329\n",
      "Epoch 1, Time Step 62, Training loss for one mini batch: 2.9486\n",
      "Epoch 1, Time Step 63, Training loss for one mini batch: 1.7116\n",
      "Epoch 1, Time Step 64, Training loss for one mini batch: 4.3328\n",
      "Epoch 1, Time Step 65, Training loss for one mini batch: 3.0716\n",
      "Epoch 1, Time Step 66, Training loss for one mini batch: 5.7926\n",
      "Epoch 1, Time Step 67, Training loss for one mini batch: 5.0659\n",
      "Epoch 1, Time Step 68, Training loss for one mini batch: 4.8611\n",
      "Epoch 1, Time Step 69, Training loss for one mini batch: 4.4541\n",
      "Epoch 1, Time Step 70, Training loss for one mini batch: 4.0643\n",
      "Epoch 1, Time Step 71, Training loss for one mini batch: 4.1335\n",
      "Epoch 1, Time Step 72, Training loss for one mini batch: 3.3692\n",
      "Epoch 1, Time Step 73, Training loss for one mini batch: 3.2177\n",
      "Epoch 1, Time Step 74, Training loss for one mini batch: 2.4125\n",
      "Epoch 1, Time Step 75, Training loss for one mini batch: 3.5729\n",
      "Epoch 1, Time Step 76, Training loss for one mini batch: 2.7525\n",
      "Epoch 1, Time Step 77, Training loss for one mini batch: 4.1107\n",
      "Epoch 1, Time Step 78, Training loss for one mini batch: 3.4859\n",
      "Epoch 1, Time Step 79, Training loss for one mini batch: 3.7794\n",
      "Epoch 1, Time Step 80, Training loss for one mini batch: 3.5338\n",
      "Epoch 1, Time Step 81, Training loss for one mini batch: 3.8316\n",
      "Epoch 1, Time Step 82, Training loss for one mini batch: 3.8873\n",
      "Epoch 1, Time Step 83, Training loss for one mini batch: 3.6131\n",
      "Epoch 1, Time Step 84, Training loss for one mini batch: 3.8555\n",
      "Epoch 1, Time Step 85, Training loss for one mini batch: 3.3792\n",
      "Epoch 1, Time Step 86, Training loss for one mini batch: 3.8283\n",
      "Epoch 1, Time Step 87, Training loss for one mini batch: 2.9816\n",
      "Epoch 1, Time Step 88, Training loss for one mini batch: 3.6421\n",
      "Epoch 1, Time Step 89, Training loss for one mini batch: 3.0389\n",
      "Epoch 1, Time Step 90, Training loss for one mini batch: 4.4959\n",
      "Epoch 1, Time Step 91, Training loss for one mini batch: 4.0342\n",
      "Epoch 1, Time Step 92, Training loss for one mini batch: 4.4957\n",
      "Epoch 1, Time Step 93, Training loss for one mini batch: 4.2182\n",
      "Epoch 1, Time Step 94, Training loss for one mini batch: 4.2562\n",
      "Epoch 1, Time Step 95, Training loss for one mini batch: 4.3714\n",
      "Epoch 1, Time Step 96, Training loss for one mini batch: 3.8282\n",
      "Epoch 1, Time Step 97, Training loss for one mini batch: 3.8037\n",
      "Epoch 1, Time Step 98, Training loss for one mini batch: 3.2866\n",
      "Epoch 1, Time Step 99, Training loss for one mini batch: 3.8699\n",
      "Epoch 1, Time Step 100, Training loss for one mini batch: 3.3376\n",
      "Epoch 1, Training Accuracy: 0.00\n",
      "\n",
      "Epoch 1, Testing Loss for last mini batch: 3.2096\n",
      "Epoch 1, Testing Accuracy: 0.00\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2, Time Step 1, Training loss for one mini batch: 4.0927\n",
      "Epoch 2, Time Step 2, Training loss for one mini batch: 3.5102\n",
      "Epoch 2, Time Step 3, Training loss for one mini batch: 3.8103\n",
      "Epoch 2, Time Step 4, Training loss for one mini batch: 3.3252\n",
      "Epoch 2, Time Step 5, Training loss for one mini batch: 3.0952\n",
      "Epoch 2, Time Step 6, Training loss for one mini batch: 2.3596\n",
      "Epoch 2, Time Step 7, Training loss for one mini batch: 2.5329\n",
      "Epoch 2, Time Step 8, Training loss for one mini batch: 3.1006\n",
      "Epoch 2, Time Step 9, Training loss for one mini batch: 2.6376\n",
      "Epoch 2, Time Step 10, Training loss for one mini batch: 3.0412\n",
      "Epoch 2, Time Step 11, Training loss for one mini batch: 2.1746\n",
      "Epoch 2, Time Step 12, Training loss for one mini batch: 3.8400\n",
      "Epoch 2, Time Step 13, Training loss for one mini batch: 2.6788\n",
      "Epoch 2, Time Step 14, Training loss for one mini batch: 4.6593\n",
      "Epoch 2, Time Step 15, Training loss for one mini batch: 3.4336\n",
      "Epoch 2, Time Step 16, Training loss for one mini batch: 3.7716\n",
      "Epoch 2, Time Step 17, Training loss for one mini batch: 2.8594\n",
      "Epoch 2, Time Step 18, Training loss for one mini batch: 3.0069\n",
      "Epoch 2, Time Step 19, Training loss for one mini batch: 2.8502\n",
      "Epoch 2, Time Step 20, Training loss for one mini batch: 2.6926\n",
      "Epoch 2, Time Step 21, Training loss for one mini batch: 3.3003\n",
      "Epoch 2, Time Step 22, Training loss for one mini batch: 2.6290\n",
      "Epoch 2, Time Step 23, Training loss for one mini batch: 3.1784\n",
      "Epoch 2, Time Step 24, Training loss for one mini batch: 2.0997\n",
      "Epoch 2, Time Step 25, Training loss for one mini batch: 4.1261\n",
      "Epoch 2, Time Step 26, Training loss for one mini batch: 2.6001\n",
      "Epoch 2, Time Step 27, Training loss for one mini batch: 4.1234\n",
      "Epoch 2, Time Step 28, Training loss for one mini batch: 2.9920\n",
      "Epoch 2, Time Step 29, Training loss for one mini batch: 3.5447\n",
      "Epoch 2, Time Step 30, Training loss for one mini batch: 2.7789\n",
      "Epoch 2, Time Step 31, Training loss for one mini batch: 2.9738\n",
      "Epoch 2, Time Step 32, Training loss for one mini batch: 2.9561\n",
      "Epoch 2, Time Step 33, Training loss for one mini batch: 2.4810\n",
      "Epoch 2, Time Step 34, Training loss for one mini batch: 2.9221\n",
      "Epoch 2, Time Step 35, Training loss for one mini batch: 2.1413\n",
      "Epoch 2, Time Step 36, Training loss for one mini batch: 2.8608\n",
      "Epoch 2, Time Step 37, Training loss for one mini batch: 1.7932\n",
      "Epoch 2, Time Step 38, Training loss for one mini batch: 3.8992\n",
      "Epoch 2, Time Step 39, Training loss for one mini batch: 2.5342\n",
      "Epoch 2, Time Step 40, Training loss for one mini batch: 4.0327\n",
      "Epoch 2, Time Step 41, Training loss for one mini batch: 3.1109\n",
      "Epoch 2, Time Step 42, Training loss for one mini batch: 3.5865\n",
      "Epoch 2, Time Step 43, Training loss for one mini batch: 2.9966\n",
      "Epoch 2, Time Step 44, Training loss for one mini batch: 2.5533\n",
      "Epoch 2, Time Step 45, Training loss for one mini batch: 2.2873\n",
      "Epoch 2, Time Step 46, Training loss for one mini batch: 1.9610\n",
      "Epoch 2, Time Step 47, Training loss for one mini batch: 2.4849\n",
      "Epoch 2, Time Step 48, Training loss for one mini batch: 1.7369\n",
      "Epoch 2, Time Step 49, Training loss for one mini batch: 3.8625\n",
      "Epoch 2, Time Step 50, Training loss for one mini batch: 2.2746\n",
      "Epoch 2, Time Step 51, Training loss for one mini batch: 4.8653\n",
      "Epoch 2, Time Step 52, Training loss for one mini batch: 3.1514\n",
      "Epoch 2, Time Step 53, Training loss for one mini batch: 3.5604\n",
      "Epoch 2, Time Step 54, Training loss for one mini batch: 2.5885\n",
      "Epoch 2, Time Step 55, Training loss for one mini batch: 2.9124\n",
      "Epoch 2, Time Step 56, Training loss for one mini batch: 2.6013\n",
      "Epoch 2, Time Step 57, Training loss for one mini batch: 2.5521\n",
      "Epoch 2, Time Step 58, Training loss for one mini batch: 2.8742\n",
      "Epoch 2, Time Step 59, Training loss for one mini batch: 2.4452\n",
      "Epoch 2, Time Step 60, Training loss for one mini batch: 3.4185\n",
      "Epoch 2, Time Step 61, Training loss for one mini batch: 2.0838\n",
      "Epoch 2, Time Step 62, Training loss for one mini batch: 2.9190\n",
      "Epoch 2, Time Step 63, Training loss for one mini batch: 1.7333\n",
      "Epoch 2, Time Step 64, Training loss for one mini batch: 4.2166\n",
      "Epoch 2, Time Step 65, Training loss for one mini batch: 2.8796\n",
      "Epoch 2, Time Step 66, Training loss for one mini batch: 4.6704\n",
      "Epoch 2, Time Step 67, Training loss for one mini batch: 3.7651\n",
      "Epoch 2, Time Step 68, Training loss for one mini batch: 3.5807\n",
      "Epoch 2, Time Step 69, Training loss for one mini batch: 3.2216\n",
      "Epoch 2, Time Step 70, Training loss for one mini batch: 2.9757\n",
      "Epoch 2, Time Step 71, Training loss for one mini batch: 3.2238\n",
      "Epoch 2, Time Step 72, Training loss for one mini batch: 2.4401\n",
      "Epoch 2, Time Step 73, Training loss for one mini batch: 2.6851\n",
      "Epoch 2, Time Step 74, Training loss for one mini batch: 1.8992\n",
      "Epoch 2, Time Step 75, Training loss for one mini batch: 3.1661\n",
      "Epoch 2, Time Step 76, Training loss for one mini batch: 2.2089\n",
      "Epoch 2, Time Step 77, Training loss for one mini batch: 3.8266\n",
      "Epoch 2, Time Step 78, Training loss for one mini batch: 3.1007\n",
      "Epoch 2, Time Step 79, Training loss for one mini batch: 3.5833\n",
      "Epoch 2, Time Step 80, Training loss for one mini batch: 3.1630\n",
      "Epoch 2, Time Step 81, Training loss for one mini batch: 3.3235\n",
      "Epoch 2, Time Step 82, Training loss for one mini batch: 3.2926\n",
      "Epoch 2, Time Step 83, Training loss for one mini batch: 2.9133\n",
      "Epoch 2, Time Step 84, Training loss for one mini batch: 3.0567\n",
      "Epoch 2, Time Step 85, Training loss for one mini batch: 2.4464\n",
      "Epoch 2, Time Step 86, Training loss for one mini batch: 2.9984\n",
      "Epoch 2, Time Step 87, Training loss for one mini batch: 2.1102\n",
      "Epoch 2, Time Step 88, Training loss for one mini batch: 3.3077\n",
      "Epoch 2, Time Step 89, Training loss for one mini batch: 2.5689\n",
      "Epoch 2, Time Step 90, Training loss for one mini batch: 4.0836\n",
      "Epoch 2, Time Step 91, Training loss for one mini batch: 3.4775\n",
      "Epoch 2, Time Step 92, Training loss for one mini batch: 3.8475\n",
      "Epoch 2, Time Step 93, Training loss for one mini batch: 3.5276\n",
      "Epoch 2, Time Step 94, Training loss for one mini batch: 3.5972\n",
      "Epoch 2, Time Step 95, Training loss for one mini batch: 3.5684\n",
      "Epoch 2, Time Step 96, Training loss for one mini batch: 2.9730\n",
      "Epoch 2, Time Step 97, Training loss for one mini batch: 3.1224\n",
      "Epoch 2, Time Step 98, Training loss for one mini batch: 2.6205\n",
      "Epoch 2, Time Step 99, Training loss for one mini batch: 3.5401\n",
      "Epoch 2, Time Step 100, Training loss for one mini batch: 2.8246\n",
      "Epoch 2, Training Accuracy: 0.00\n",
      "\n",
      "Epoch 2, Testing Loss for last mini batch: 3.2096\n",
      "Epoch 2, Testing Accuracy: 0.00\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3, Time Step 1, Training loss for one mini batch: 3.9177\n",
      "Epoch 3, Time Step 2, Training loss for one mini batch: 3.1586\n",
      "Epoch 3, Time Step 3, Training loss for one mini batch: 3.5916\n",
      "Epoch 3, Time Step 4, Training loss for one mini batch: 2.7859\n",
      "Epoch 3, Time Step 5, Training loss for one mini batch: 2.5435\n",
      "Epoch 3, Time Step 6, Training loss for one mini batch: 1.9633\n",
      "Epoch 3, Time Step 7, Training loss for one mini batch: 2.3507\n",
      "Epoch 3, Time Step 8, Training loss for one mini batch: 2.9007\n",
      "Epoch 3, Time Step 9, Training loss for one mini batch: 2.3553\n",
      "Epoch 3, Time Step 10, Training loss for one mini batch: 2.8043\n",
      "Epoch 3, Time Step 11, Training loss for one mini batch: 1.9420\n",
      "Epoch 3, Time Step 12, Training loss for one mini batch: 3.5661\n",
      "Epoch 3, Time Step 13, Training loss for one mini batch: 2.3616\n",
      "Epoch 3, Time Step 14, Training loss for one mini batch: 4.4441\n",
      "Epoch 3, Time Step 15, Training loss for one mini batch: 3.0116\n",
      "Epoch 3, Time Step 16, Training loss for one mini batch: 3.6895\n",
      "Epoch 3, Time Step 17, Training loss for one mini batch: 2.6929\n",
      "Epoch 3, Time Step 18, Training loss for one mini batch: 2.8745\n",
      "Epoch 3, Time Step 19, Training loss for one mini batch: 2.5665\n",
      "Epoch 3, Time Step 20, Training loss for one mini batch: 2.4672\n",
      "Epoch 3, Time Step 21, Training loss for one mini batch: 3.1762\n",
      "Epoch 3, Time Step 22, Training loss for one mini batch: 2.3412\n",
      "Epoch 3, Time Step 23, Training loss for one mini batch: 2.6938\n",
      "Epoch 3, Time Step 24, Training loss for one mini batch: 1.6206\n",
      "Epoch 3, Time Step 25, Training loss for one mini batch: 3.8905\n",
      "Epoch 3, Time Step 26, Training loss for one mini batch: 2.2456\n",
      "Epoch 3, Time Step 27, Training loss for one mini batch: 3.9128\n",
      "Epoch 3, Time Step 28, Training loss for one mini batch: 2.6895\n",
      "Epoch 3, Time Step 29, Training loss for one mini batch: 3.1790\n",
      "Epoch 3, Time Step 30, Training loss for one mini batch: 2.2915\n",
      "Epoch 3, Time Step 31, Training loss for one mini batch: 2.5421\n",
      "Epoch 3, Time Step 32, Training loss for one mini batch: 2.4903\n",
      "Epoch 3, Time Step 33, Training loss for one mini batch: 2.1519\n",
      "Epoch 3, Time Step 34, Training loss for one mini batch: 2.7415\n",
      "Epoch 3, Time Step 35, Training loss for one mini batch: 1.9538\n",
      "Epoch 3, Time Step 36, Training loss for one mini batch: 2.6555\n",
      "Epoch 3, Time Step 37, Training loss for one mini batch: 1.5037\n",
      "Epoch 3, Time Step 38, Training loss for one mini batch: 3.5738\n",
      "Epoch 3, Time Step 39, Training loss for one mini batch: 2.1006\n",
      "Epoch 3, Time Step 40, Training loss for one mini batch: 3.9203\n",
      "Epoch 3, Time Step 41, Training loss for one mini batch: 2.8955\n",
      "Epoch 3, Time Step 42, Training loss for one mini batch: 3.3173\n",
      "Epoch 3, Time Step 43, Training loss for one mini batch: 2.5325\n",
      "Epoch 3, Time Step 44, Training loss for one mini batch: 2.2227\n",
      "Epoch 3, Time Step 45, Training loss for one mini batch: 2.1423\n",
      "Epoch 3, Time Step 46, Training loss for one mini batch: 1.8325\n",
      "Epoch 3, Time Step 47, Training loss for one mini batch: 2.3940\n",
      "Epoch 3, Time Step 48, Training loss for one mini batch: 1.5603\n",
      "Epoch 3, Time Step 49, Training loss for one mini batch: 3.5609\n",
      "Epoch 3, Time Step 50, Training loss for one mini batch: 1.8625\n",
      "Epoch 3, Time Step 51, Training loss for one mini batch: 4.6151\n",
      "Epoch 3, Time Step 52, Training loss for one mini batch: 2.7999\n",
      "Epoch 3, Time Step 53, Training loss for one mini batch: 3.1264\n",
      "Epoch 3, Time Step 54, Training loss for one mini batch: 2.0555\n",
      "Epoch 3, Time Step 55, Training loss for one mini batch: 2.5666\n",
      "Epoch 3, Time Step 56, Training loss for one mini batch: 2.2915\n",
      "Epoch 3, Time Step 57, Training loss for one mini batch: 2.2871\n",
      "Epoch 3, Time Step 58, Training loss for one mini batch: 2.5059\n",
      "Epoch 3, Time Step 59, Training loss for one mini batch: 2.1575\n",
      "Epoch 3, Time Step 60, Training loss for one mini batch: 3.2577\n",
      "Epoch 3, Time Step 61, Training loss for one mini batch: 1.7928\n",
      "Epoch 3, Time Step 62, Training loss for one mini batch: 2.8077\n",
      "Epoch 3, Time Step 63, Training loss for one mini batch: 1.5805\n",
      "Epoch 3, Time Step 64, Training loss for one mini batch: 4.2882\n",
      "Epoch 3, Time Step 65, Training loss for one mini batch: 2.8554\n",
      "Epoch 3, Time Step 66, Training loss for one mini batch: 4.0358\n",
      "Epoch 3, Time Step 67, Training loss for one mini batch: 2.9310\n",
      "Epoch 3, Time Step 68, Training loss for one mini batch: 2.9809\n",
      "Epoch 3, Time Step 69, Training loss for one mini batch: 2.7184\n",
      "Epoch 3, Time Step 70, Training loss for one mini batch: 2.5315\n",
      "Epoch 3, Time Step 71, Training loss for one mini batch: 2.9657\n",
      "Epoch 3, Time Step 72, Training loss for one mini batch: 2.1068\n",
      "Epoch 3, Time Step 73, Training loss for one mini batch: 2.5350\n",
      "Epoch 3, Time Step 74, Training loss for one mini batch: 1.6804\n",
      "Epoch 3, Time Step 75, Training loss for one mini batch: 3.1342\n",
      "Epoch 3, Time Step 76, Training loss for one mini batch: 1.9659\n",
      "Epoch 3, Time Step 77, Training loss for one mini batch: 3.9714\n",
      "Epoch 3, Time Step 78, Training loss for one mini batch: 3.0724\n",
      "Epoch 3, Time Step 79, Training loss for one mini batch: 3.5335\n",
      "Epoch 3, Time Step 80, Training loss for one mini batch: 2.9757\n",
      "Epoch 3, Time Step 81, Training loss for one mini batch: 3.0700\n",
      "Epoch 3, Time Step 82, Training loss for one mini batch: 3.0235\n",
      "Epoch 3, Time Step 83, Training loss for one mini batch: 2.5426\n",
      "Epoch 3, Time Step 84, Training loss for one mini batch: 2.6757\n",
      "Epoch 3, Time Step 85, Training loss for one mini batch: 2.0130\n",
      "Epoch 3, Time Step 86, Training loss for one mini batch: 2.5558\n",
      "Epoch 3, Time Step 87, Training loss for one mini batch: 1.6857\n",
      "Epoch 3, Time Step 88, Training loss for one mini batch: 3.3138\n",
      "Epoch 3, Time Step 89, Training loss for one mini batch: 2.4435\n",
      "Epoch 3, Time Step 90, Training loss for one mini batch: 3.9000\n",
      "Epoch 3, Time Step 91, Training loss for one mini batch: 3.1525\n",
      "Epoch 3, Time Step 92, Training loss for one mini batch: 3.4654\n",
      "Epoch 3, Time Step 93, Training loss for one mini batch: 3.1243\n",
      "Epoch 3, Time Step 94, Training loss for one mini batch: 3.2238\n",
      "Epoch 3, Time Step 95, Training loss for one mini batch: 3.1917\n",
      "Epoch 3, Time Step 96, Training loss for one mini batch: 2.5628\n",
      "Epoch 3, Time Step 97, Training loss for one mini batch: 2.7732\n",
      "Epoch 3, Time Step 98, Training loss for one mini batch: 2.2597\n",
      "Epoch 3, Time Step 99, Training loss for one mini batch: 3.2433\n",
      "Epoch 3, Time Step 100, Training loss for one mini batch: 2.4463\n",
      "Epoch 3, Training Accuracy: 0.00\n",
      "\n",
      "Epoch 3, Testing Loss for last mini batch: 3.2096\n",
      "Epoch 3, Testing Accuracy: 0.00\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4, Time Step 1, Training loss for one mini batch: 3.7618\n",
      "Epoch 4, Time Step 2, Training loss for one mini batch: 2.9328\n",
      "Epoch 4, Time Step 3, Training loss for one mini batch: 3.2181\n",
      "Epoch 4, Time Step 4, Training loss for one mini batch: 2.3015\n",
      "Epoch 4, Time Step 5, Training loss for one mini batch: 2.2136\n",
      "Epoch 4, Time Step 6, Training loss for one mini batch: 1.7863\n",
      "Epoch 4, Time Step 7, Training loss for one mini batch: 2.2430\n",
      "Epoch 4, Time Step 8, Training loss for one mini batch: 2.7109\n",
      "Epoch 4, Time Step 9, Training loss for one mini batch: 2.1914\n",
      "Epoch 4, Time Step 10, Training loss for one mini batch: 2.7672\n",
      "Epoch 4, Time Step 11, Training loss for one mini batch: 1.8975\n",
      "Epoch 4, Time Step 12, Training loss for one mini batch: 3.3312\n",
      "Epoch 4, Time Step 13, Training loss for one mini batch: 2.0992\n",
      "Epoch 4, Time Step 14, Training loss for one mini batch: 4.1404\n",
      "Epoch 4, Time Step 15, Training loss for one mini batch: 2.5977\n",
      "Epoch 4, Time Step 16, Training loss for one mini batch: 3.4731\n",
      "Epoch 4, Time Step 17, Training loss for one mini batch: 2.4978\n",
      "Epoch 4, Time Step 18, Training loss for one mini batch: 2.6864\n",
      "Epoch 4, Time Step 19, Training loss for one mini batch: 2.2768\n",
      "Epoch 4, Time Step 20, Training loss for one mini batch: 2.2159\n",
      "Epoch 4, Time Step 21, Training loss for one mini batch: 2.9164\n",
      "Epoch 4, Time Step 22, Training loss for one mini batch: 2.0616\n",
      "Epoch 4, Time Step 23, Training loss for one mini batch: 2.4063\n",
      "Epoch 4, Time Step 24, Training loss for one mini batch: 1.3627\n",
      "Epoch 4, Time Step 25, Training loss for one mini batch: 3.5197\n",
      "Epoch 4, Time Step 26, Training loss for one mini batch: 1.8730\n",
      "Epoch 4, Time Step 27, Training loss for one mini batch: 3.7668\n",
      "Epoch 4, Time Step 28, Training loss for one mini batch: 2.4939\n",
      "Epoch 4, Time Step 29, Training loss for one mini batch: 2.9692\n",
      "Epoch 4, Time Step 30, Training loss for one mini batch: 2.0175\n",
      "Epoch 4, Time Step 31, Training loss for one mini batch: 2.2456\n",
      "Epoch 4, Time Step 32, Training loss for one mini batch: 2.0533\n",
      "Epoch 4, Time Step 33, Training loss for one mini batch: 1.8171\n",
      "Epoch 4, Time Step 34, Training loss for one mini batch: 2.5665\n",
      "Epoch 4, Time Step 35, Training loss for one mini batch: 1.8282\n",
      "Epoch 4, Time Step 36, Training loss for one mini batch: 2.6137\n",
      "Epoch 4, Time Step 37, Training loss for one mini batch: 1.3880\n",
      "Epoch 4, Time Step 38, Training loss for one mini batch: 3.3615\n",
      "Epoch 4, Time Step 39, Training loss for one mini batch: 1.8451\n",
      "Epoch 4, Time Step 40, Training loss for one mini batch: 3.8551\n",
      "Epoch 4, Time Step 41, Training loss for one mini batch: 2.7351\n",
      "Epoch 4, Time Step 42, Training loss for one mini batch: 3.0884\n",
      "Epoch 4, Time Step 43, Training loss for one mini batch: 2.1264\n",
      "Epoch 4, Time Step 44, Training loss for one mini batch: 1.9660\n",
      "Epoch 4, Time Step 45, Training loss for one mini batch: 2.0324\n",
      "Epoch 4, Time Step 46, Training loss for one mini batch: 1.6785\n",
      "Epoch 4, Time Step 47, Training loss for one mini batch: 2.1295\n",
      "Epoch 4, Time Step 48, Training loss for one mini batch: 1.2964\n",
      "Epoch 4, Time Step 49, Training loss for one mini batch: 3.0713\n",
      "Epoch 4, Time Step 50, Training loss for one mini batch: 1.3341\n",
      "Epoch 4, Time Step 51, Training loss for one mini batch: 4.6559\n",
      "Epoch 4, Time Step 52, Training loss for one mini batch: 2.7633\n",
      "Epoch 4, Time Step 53, Training loss for one mini batch: 3.0524\n",
      "Epoch 4, Time Step 54, Training loss for one mini batch: 1.8775\n",
      "Epoch 4, Time Step 55, Training loss for one mini batch: 2.4242\n",
      "Epoch 4, Time Step 56, Training loss for one mini batch: 2.0583\n",
      "Epoch 4, Time Step 57, Training loss for one mini batch: 2.0742\n",
      "Epoch 4, Time Step 58, Training loss for one mini batch: 2.2377\n",
      "Epoch 4, Time Step 59, Training loss for one mini batch: 1.9817\n",
      "Epoch 4, Time Step 60, Training loss for one mini batch: 3.1676\n",
      "Epoch 4, Time Step 61, Training loss for one mini batch: 1.5973\n",
      "Epoch 4, Time Step 62, Training loss for one mini batch: 2.5911\n",
      "Epoch 4, Time Step 63, Training loss for one mini batch: 1.2636\n",
      "Epoch 4, Time Step 64, Training loss for one mini batch: 4.3871\n",
      "Epoch 4, Time Step 65, Training loss for one mini batch: 2.8666\n",
      "Epoch 4, Time Step 66, Training loss for one mini batch: 3.7339\n",
      "Epoch 4, Time Step 67, Training loss for one mini batch: 2.4339\n",
      "Epoch 4, Time Step 68, Training loss for one mini batch: 2.6364\n",
      "Epoch 4, Time Step 69, Training loss for one mini batch: 2.3388\n",
      "Epoch 4, Time Step 70, Training loss for one mini batch: 2.2716\n",
      "Epoch 4, Time Step 71, Training loss for one mini batch: 2.8823\n",
      "Epoch 4, Time Step 72, Training loss for one mini batch: 1.9551\n",
      "Epoch 4, Time Step 73, Training loss for one mini batch: 2.4355\n",
      "Epoch 4, Time Step 74, Training loss for one mini batch: 1.5201\n",
      "Epoch 4, Time Step 75, Training loss for one mini batch: 3.1563\n",
      "Epoch 4, Time Step 76, Training loss for one mini batch: 1.8304\n",
      "Epoch 4, Time Step 77, Training loss for one mini batch: 4.1222\n",
      "Epoch 4, Time Step 78, Training loss for one mini batch: 3.0933\n",
      "Epoch 4, Time Step 79, Training loss for one mini batch: 3.5139\n",
      "Epoch 4, Time Step 80, Training loss for one mini batch: 2.8138\n",
      "Epoch 4, Time Step 81, Training loss for one mini batch: 2.8496\n",
      "Epoch 4, Time Step 82, Training loss for one mini batch: 2.7539\n",
      "Epoch 4, Time Step 83, Training loss for one mini batch: 2.2099\n",
      "Epoch 4, Time Step 84, Training loss for one mini batch: 2.3796\n",
      "Epoch 4, Time Step 85, Training loss for one mini batch: 1.7216\n",
      "Epoch 4, Time Step 86, Training loss for one mini batch: 2.4649\n",
      "Epoch 4, Time Step 87, Training loss for one mini batch: 1.5349\n",
      "Epoch 4, Time Step 88, Training loss for one mini batch: 3.3393\n",
      "Epoch 4, Time Step 89, Training loss for one mini batch: 2.3989\n",
      "Epoch 4, Time Step 90, Training loss for one mini batch: 3.7754\n",
      "Epoch 4, Time Step 91, Training loss for one mini batch: 2.9076\n",
      "Epoch 4, Time Step 92, Training loss for one mini batch: 3.2222\n",
      "Epoch 4, Time Step 93, Training loss for one mini batch: 2.8316\n",
      "Epoch 4, Time Step 94, Training loss for one mini batch: 2.9831\n",
      "Epoch 4, Time Step 95, Training loss for one mini batch: 2.9952\n",
      "Epoch 4, Time Step 96, Training loss for one mini batch: 2.3441\n",
      "Epoch 4, Time Step 97, Training loss for one mini batch: 2.5997\n",
      "Epoch 4, Time Step 98, Training loss for one mini batch: 2.0530\n",
      "Epoch 4, Time Step 99, Training loss for one mini batch: 3.0802\n",
      "Epoch 4, Time Step 100, Training loss for one mini batch: 2.2427\n",
      "Epoch 4, Training Accuracy: 0.00\n",
      "\n",
      "Epoch 4, Testing Loss for last mini batch: 3.2096\n",
      "Epoch 4, Testing Accuracy: 0.00\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5, Time Step 1, Training loss for one mini batch: 3.6167\n",
      "Epoch 5, Time Step 2, Training loss for one mini batch: 2.7145\n",
      "Epoch 5, Time Step 3, Training loss for one mini batch: 3.0775\n",
      "Epoch 5, Time Step 4, Training loss for one mini batch: 2.1187\n",
      "Epoch 5, Time Step 5, Training loss for one mini batch: 2.0571\n",
      "Epoch 5, Time Step 6, Training loss for one mini batch: 1.6758\n",
      "Epoch 5, Time Step 7, Training loss for one mini batch: 2.1361\n",
      "Epoch 5, Time Step 8, Training loss for one mini batch: 2.5985\n",
      "Epoch 5, Time Step 9, Training loss for one mini batch: 2.1234\n",
      "Epoch 5, Time Step 10, Training loss for one mini batch: 2.7326\n",
      "Epoch 5, Time Step 11, Training loss for one mini batch: 1.8355\n",
      "Epoch 5, Time Step 12, Training loss for one mini batch: 3.0718\n",
      "Epoch 5, Time Step 13, Training loss for one mini batch: 1.8076\n",
      "Epoch 5, Time Step 14, Training loss for one mini batch: 3.9751\n",
      "Epoch 5, Time Step 15, Training loss for one mini batch: 2.4166\n",
      "Epoch 5, Time Step 16, Training loss for one mini batch: 3.2739\n",
      "Epoch 5, Time Step 17, Training loss for one mini batch: 2.2165\n",
      "Epoch 5, Time Step 18, Training loss for one mini batch: 2.4462\n",
      "Epoch 5, Time Step 19, Training loss for one mini batch: 1.9946\n",
      "Epoch 5, Time Step 20, Training loss for one mini batch: 2.0123\n",
      "Epoch 5, Time Step 21, Training loss for one mini batch: 2.6988\n",
      "Epoch 5, Time Step 22, Training loss for one mini batch: 1.8625\n",
      "Epoch 5, Time Step 23, Training loss for one mini batch: 2.2292\n",
      "Epoch 5, Time Step 24, Training loss for one mini batch: 1.2342\n",
      "Epoch 5, Time Step 25, Training loss for one mini batch: 3.1438\n",
      "Epoch 5, Time Step 26, Training loss for one mini batch: 1.6023\n",
      "Epoch 5, Time Step 27, Training loss for one mini batch: 3.6418\n",
      "Epoch 5, Time Step 28, Training loss for one mini batch: 2.3540\n",
      "Epoch 5, Time Step 29, Training loss for one mini batch: 2.9169\n",
      "Epoch 5, Time Step 30, Training loss for one mini batch: 1.9701\n",
      "Epoch 5, Time Step 31, Training loss for one mini batch: 2.1235\n",
      "Epoch 5, Time Step 32, Training loss for one mini batch: 1.8248\n",
      "Epoch 5, Time Step 33, Training loss for one mini batch: 1.7021\n",
      "Epoch 5, Time Step 34, Training loss for one mini batch: 2.4512\n",
      "Epoch 5, Time Step 35, Training loss for one mini batch: 1.6646\n",
      "Epoch 5, Time Step 36, Training loss for one mini batch: 2.5122\n",
      "Epoch 5, Time Step 37, Training loss for one mini batch: 1.2674\n",
      "Epoch 5, Time Step 38, Training loss for one mini batch: 3.2071\n",
      "Epoch 5, Time Step 39, Training loss for one mini batch: 1.6526\n",
      "Epoch 5, Time Step 40, Training loss for one mini batch: 3.7770\n",
      "Epoch 5, Time Step 41, Training loss for one mini batch: 2.5945\n",
      "Epoch 5, Time Step 42, Training loss for one mini batch: 2.9358\n",
      "Epoch 5, Time Step 43, Training loss for one mini batch: 1.8623\n",
      "Epoch 5, Time Step 44, Training loss for one mini batch: 1.8222\n",
      "Epoch 5, Time Step 45, Training loss for one mini batch: 1.9125\n",
      "Epoch 5, Time Step 46, Training loss for one mini batch: 1.5643\n",
      "Epoch 5, Time Step 47, Training loss for one mini batch: 1.9802\n",
      "Epoch 5, Time Step 48, Training loss for one mini batch: 1.1599\n",
      "Epoch 5, Time Step 49, Training loss for one mini batch: 2.7902\n",
      "Epoch 5, Time Step 50, Training loss for one mini batch: 1.0819\n",
      "Epoch 5, Time Step 51, Training loss for one mini batch: 4.6623\n",
      "Epoch 5, Time Step 52, Training loss for one mini batch: 2.6654\n",
      "Epoch 5, Time Step 53, Training loss for one mini batch: 2.9110\n",
      "Epoch 5, Time Step 54, Training loss for one mini batch: 1.6793\n",
      "Epoch 5, Time Step 55, Training loss for one mini batch: 2.3350\n",
      "Epoch 5, Time Step 56, Training loss for one mini batch: 1.9271\n",
      "Epoch 5, Time Step 57, Training loss for one mini batch: 1.9586\n",
      "Epoch 5, Time Step 58, Training loss for one mini batch: 2.1407\n",
      "Epoch 5, Time Step 59, Training loss for one mini batch: 1.8513\n",
      "Epoch 5, Time Step 60, Training loss for one mini batch: 3.0077\n",
      "Epoch 5, Time Step 61, Training loss for one mini batch: 1.4379\n",
      "Epoch 5, Time Step 62, Training loss for one mini batch: 2.4831\n",
      "Epoch 5, Time Step 63, Training loss for one mini batch: 1.0627\n",
      "Epoch 5, Time Step 64, Training loss for one mini batch: 4.4746\n",
      "Epoch 5, Time Step 65, Training loss for one mini batch: 2.8680\n",
      "Epoch 5, Time Step 66, Training loss for one mini batch: 3.5979\n",
      "Epoch 5, Time Step 67, Training loss for one mini batch: 2.1742\n",
      "Epoch 5, Time Step 68, Training loss for one mini batch: 2.3712\n",
      "Epoch 5, Time Step 69, Training loss for one mini batch: 1.9876\n",
      "Epoch 5, Time Step 70, Training loss for one mini batch: 2.1159\n",
      "Epoch 5, Time Step 71, Training loss for one mini batch: 2.8043\n",
      "Epoch 5, Time Step 72, Training loss for one mini batch: 1.8286\n",
      "Epoch 5, Time Step 73, Training loss for one mini batch: 2.3939\n",
      "Epoch 5, Time Step 74, Training loss for one mini batch: 1.4585\n",
      "Epoch 5, Time Step 75, Training loss for one mini batch: 3.1209\n",
      "Epoch 5, Time Step 76, Training loss for one mini batch: 1.6996\n",
      "Epoch 5, Time Step 77, Training loss for one mini batch: 4.1801\n",
      "Epoch 5, Time Step 78, Training loss for one mini batch: 3.0442\n",
      "Epoch 5, Time Step 79, Training loss for one mini batch: 3.4012\n",
      "Epoch 5, Time Step 80, Training loss for one mini batch: 2.5939\n",
      "Epoch 5, Time Step 81, Training loss for one mini batch: 2.6370\n",
      "Epoch 5, Time Step 82, Training loss for one mini batch: 2.5694\n",
      "Epoch 5, Time Step 83, Training loss for one mini batch: 2.0840\n",
      "Epoch 5, Time Step 84, Training loss for one mini batch: 2.2105\n",
      "Epoch 5, Time Step 85, Training loss for one mini batch: 1.5867\n",
      "Epoch 5, Time Step 86, Training loss for one mini batch: 2.4577\n",
      "Epoch 5, Time Step 87, Training loss for one mini batch: 1.4950\n",
      "Epoch 5, Time Step 88, Training loss for one mini batch: 3.2958\n",
      "Epoch 5, Time Step 89, Training loss for one mini batch: 2.3047\n",
      "Epoch 5, Time Step 90, Training loss for one mini batch: 3.7267\n",
      "Epoch 5, Time Step 91, Training loss for one mini batch: 2.8125\n",
      "Epoch 5, Time Step 92, Training loss for one mini batch: 3.0327\n",
      "Epoch 5, Time Step 93, Training loss for one mini batch: 2.5620\n",
      "Epoch 5, Time Step 94, Training loss for one mini batch: 2.7609\n",
      "Epoch 5, Time Step 95, Training loss for one mini batch: 2.8139\n",
      "Epoch 5, Time Step 96, Training loss for one mini batch: 2.2084\n",
      "Epoch 5, Time Step 97, Training loss for one mini batch: 2.5333\n",
      "Epoch 5, Time Step 98, Training loss for one mini batch: 1.9206\n",
      "Epoch 5, Time Step 99, Training loss for one mini batch: 2.9702\n",
      "Epoch 5, Time Step 100, Training loss for one mini batch: 2.1203\n",
      "Epoch 5, Training Accuracy: 0.00\n",
      "\n",
      "Epoch 5, Testing Loss for last mini batch: 3.2096\n",
      "Epoch 5, Testing Accuracy: 0.00\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6, Time Step 1, Training loss for one mini batch: 3.4859\n",
      "Epoch 6, Time Step 2, Training loss for one mini batch: 2.5266\n",
      "Epoch 6, Time Step 3, Training loss for one mini batch: 2.9366\n",
      "Epoch 6, Time Step 4, Training loss for one mini batch: 2.0396\n",
      "Epoch 6, Time Step 5, Training loss for one mini batch: 1.9947\n",
      "Epoch 6, Time Step 6, Training loss for one mini batch: 1.5975\n",
      "Epoch 6, Time Step 7, Training loss for one mini batch: 2.0706\n",
      "Epoch 6, Time Step 8, Training loss for one mini batch: 2.5167\n",
      "Epoch 6, Time Step 9, Training loss for one mini batch: 2.0606\n",
      "Epoch 6, Time Step 10, Training loss for one mini batch: 2.7259\n",
      "Epoch 6, Time Step 11, Training loss for one mini batch: 1.8335\n",
      "Epoch 6, Time Step 12, Training loss for one mini batch: 2.9392\n",
      "Epoch 6, Time Step 13, Training loss for one mini batch: 1.6502\n",
      "Epoch 6, Time Step 14, Training loss for one mini batch: 3.7457\n",
      "Epoch 6, Time Step 15, Training loss for one mini batch: 2.1471\n",
      "Epoch 6, Time Step 16, Training loss for one mini batch: 3.1085\n",
      "Epoch 6, Time Step 17, Training loss for one mini batch: 2.0871\n",
      "Epoch 6, Time Step 18, Training loss for one mini batch: 2.4044\n",
      "Epoch 6, Time Step 19, Training loss for one mini batch: 1.9788\n",
      "Epoch 6, Time Step 20, Training loss for one mini batch: 1.9468\n",
      "Epoch 6, Time Step 21, Training loss for one mini batch: 2.5488\n",
      "Epoch 6, Time Step 22, Training loss for one mini batch: 1.7615\n",
      "Epoch 6, Time Step 23, Training loss for one mini batch: 2.1375\n",
      "Epoch 6, Time Step 24, Training loss for one mini batch: 1.1562\n",
      "Epoch 6, Time Step 25, Training loss for one mini batch: 3.0261\n",
      "Epoch 6, Time Step 26, Training loss for one mini batch: 1.4760\n",
      "Epoch 6, Time Step 27, Training loss for one mini batch: 3.4357\n",
      "Epoch 6, Time Step 28, Training loss for one mini batch: 2.1151\n",
      "Epoch 6, Time Step 29, Training loss for one mini batch: 2.7900\n",
      "Epoch 6, Time Step 30, Training loss for one mini batch: 1.9074\n",
      "Epoch 6, Time Step 31, Training loss for one mini batch: 2.0670\n",
      "Epoch 6, Time Step 32, Training loss for one mini batch: 1.7539\n",
      "Epoch 6, Time Step 33, Training loss for one mini batch: 1.6581\n",
      "Epoch 6, Time Step 34, Training loss for one mini batch: 2.3547\n",
      "Epoch 6, Time Step 35, Training loss for one mini batch: 1.5795\n",
      "Epoch 6, Time Step 36, Training loss for one mini batch: 2.4659\n",
      "Epoch 6, Time Step 37, Training loss for one mini batch: 1.2067\n",
      "Epoch 6, Time Step 38, Training loss for one mini batch: 3.0727\n",
      "Epoch 6, Time Step 39, Training loss for one mini batch: 1.5205\n",
      "Epoch 6, Time Step 40, Training loss for one mini batch: 3.6514\n",
      "Epoch 6, Time Step 41, Training loss for one mini batch: 2.4232\n",
      "Epoch 6, Time Step 42, Training loss for one mini batch: 2.8144\n",
      "Epoch 6, Time Step 43, Training loss for one mini batch: 1.7205\n",
      "Epoch 6, Time Step 44, Training loss for one mini batch: 1.7676\n",
      "Epoch 6, Time Step 45, Training loss for one mini batch: 1.8383\n",
      "Epoch 6, Time Step 46, Training loss for one mini batch: 1.5112\n",
      "Epoch 6, Time Step 47, Training loss for one mini batch: 1.9097\n",
      "Epoch 6, Time Step 48, Training loss for one mini batch: 1.0991\n",
      "Epoch 6, Time Step 49, Training loss for one mini batch: 2.6052\n",
      "Epoch 6, Time Step 50, Training loss for one mini batch: 0.9673\n",
      "Epoch 6, Time Step 51, Training loss for one mini batch: 4.6072\n",
      "Epoch 6, Time Step 52, Training loss for one mini batch: 2.5609\n",
      "Epoch 6, Time Step 53, Training loss for one mini batch: 2.7728\n",
      "Epoch 6, Time Step 54, Training loss for one mini batch: 1.5078\n",
      "Epoch 6, Time Step 55, Training loss for one mini batch: 2.2802\n",
      "Epoch 6, Time Step 56, Training loss for one mini batch: 1.8517\n",
      "Epoch 6, Time Step 57, Training loss for one mini batch: 1.9315\n",
      "Epoch 6, Time Step 58, Training loss for one mini batch: 2.1478\n",
      "Epoch 6, Time Step 59, Training loss for one mini batch: 1.7901\n",
      "Epoch 6, Time Step 60, Training loss for one mini batch: 2.8593\n",
      "Epoch 6, Time Step 61, Training loss for one mini batch: 1.3145\n",
      "Epoch 6, Time Step 62, Training loss for one mini batch: 2.4858\n",
      "Epoch 6, Time Step 63, Training loss for one mini batch: 1.0183\n",
      "Epoch 6, Time Step 64, Training loss for one mini batch: 4.4764\n",
      "Epoch 6, Time Step 65, Training loss for one mini batch: 2.8065\n",
      "Epoch 6, Time Step 66, Training loss for one mini batch: 3.4208\n",
      "Epoch 6, Time Step 67, Training loss for one mini batch: 2.0061\n",
      "Epoch 6, Time Step 68, Training loss for one mini batch: 2.2149\n",
      "Epoch 6, Time Step 69, Training loss for one mini batch: 1.8338\n",
      "Epoch 6, Time Step 70, Training loss for one mini batch: 2.0614\n",
      "Epoch 6, Time Step 71, Training loss for one mini batch: 2.7404\n",
      "Epoch 6, Time Step 72, Training loss for one mini batch: 1.7512\n",
      "Epoch 6, Time Step 73, Training loss for one mini batch: 2.3812\n",
      "Epoch 6, Time Step 74, Training loss for one mini batch: 1.4374\n",
      "Epoch 6, Time Step 75, Training loss for one mini batch: 3.1140\n",
      "Epoch 6, Time Step 76, Training loss for one mini batch: 1.6470\n",
      "Epoch 6, Time Step 77, Training loss for one mini batch: 4.1326\n",
      "Epoch 6, Time Step 78, Training loss for one mini batch: 2.9037\n",
      "Epoch 6, Time Step 79, Training loss for one mini batch: 3.2531\n",
      "Epoch 6, Time Step 80, Training loss for one mini batch: 2.3950\n",
      "Epoch 6, Time Step 81, Training loss for one mini batch: 2.4995\n",
      "Epoch 6, Time Step 82, Training loss for one mini batch: 2.4800\n",
      "Epoch 6, Time Step 83, Training loss for one mini batch: 2.0159\n",
      "Epoch 6, Time Step 84, Training loss for one mini batch: 2.1542\n",
      "Epoch 6, Time Step 85, Training loss for one mini batch: 1.5470\n",
      "Epoch 6, Time Step 86, Training loss for one mini batch: 2.4451\n",
      "Epoch 6, Time Step 87, Training loss for one mini batch: 1.4637\n",
      "Epoch 6, Time Step 88, Training loss for one mini batch: 3.2465\n",
      "Epoch 6, Time Step 89, Training loss for one mini batch: 2.2256\n",
      "Epoch 6, Time Step 90, Training loss for one mini batch: 3.6537\n",
      "Epoch 6, Time Step 91, Training loss for one mini batch: 2.7045\n",
      "Epoch 6, Time Step 92, Training loss for one mini batch: 2.8698\n",
      "Epoch 6, Time Step 93, Training loss for one mini batch: 2.3650\n",
      "Epoch 6, Time Step 94, Training loss for one mini batch: 2.5901\n",
      "Epoch 6, Time Step 95, Training loss for one mini batch: 2.6546\n",
      "Epoch 6, Time Step 96, Training loss for one mini batch: 2.1220\n",
      "Epoch 6, Time Step 97, Training loss for one mini batch: 2.5017\n",
      "Epoch 6, Time Step 98, Training loss for one mini batch: 1.8268\n",
      "Epoch 6, Time Step 99, Training loss for one mini batch: 2.8965\n",
      "Epoch 6, Time Step 100, Training loss for one mini batch: 2.0420\n",
      "Epoch 6, Training Accuracy: 0.00\n",
      "\n",
      "Epoch 6, Testing Loss for last mini batch: 3.2096\n",
      "Epoch 6, Testing Accuracy: 0.00\n",
      "\n",
      "\n",
      "\n",
      "Epoch 7, Time Step 1, Training loss for one mini batch: 3.3393\n",
      "Epoch 7, Time Step 2, Training loss for one mini batch: 2.3685\n",
      "Epoch 7, Time Step 3, Training loss for one mini batch: 2.8804\n",
      "Epoch 7, Time Step 4, Training loss for one mini batch: 2.0110\n",
      "Epoch 7, Time Step 5, Training loss for one mini batch: 1.9584\n",
      "Epoch 7, Time Step 6, Training loss for one mini batch: 1.5324\n",
      "Epoch 7, Time Step 7, Training loss for one mini batch: 2.0109\n",
      "Epoch 7, Time Step 8, Training loss for one mini batch: 2.4609\n",
      "Epoch 7, Time Step 9, Training loss for one mini batch: 2.0284\n",
      "Epoch 7, Time Step 10, Training loss for one mini batch: 2.6681\n",
      "Epoch 7, Time Step 11, Training loss for one mini batch: 1.7966\n",
      "Epoch 7, Time Step 12, Training loss for one mini batch: 2.8542\n",
      "Epoch 7, Time Step 13, Training loss for one mini batch: 1.5756\n",
      "Epoch 7, Time Step 14, Training loss for one mini batch: 3.6642\n",
      "Epoch 7, Time Step 15, Training loss for one mini batch: 2.0593\n",
      "Epoch 7, Time Step 16, Training loss for one mini batch: 3.0691\n",
      "Epoch 7, Time Step 17, Training loss for one mini batch: 2.0280\n",
      "Epoch 7, Time Step 18, Training loss for one mini batch: 2.3106\n",
      "Epoch 7, Time Step 19, Training loss for one mini batch: 1.8657\n",
      "Epoch 7, Time Step 20, Training loss for one mini batch: 1.8560\n",
      "Epoch 7, Time Step 21, Training loss for one mini batch: 2.4367\n",
      "Epoch 7, Time Step 22, Training loss for one mini batch: 1.7169\n",
      "Epoch 7, Time Step 23, Training loss for one mini batch: 2.1248\n",
      "Epoch 7, Time Step 24, Training loss for one mini batch: 1.1428\n",
      "Epoch 7, Time Step 25, Training loss for one mini batch: 2.9156\n",
      "Epoch 7, Time Step 26, Training loss for one mini batch: 1.3797\n",
      "Epoch 7, Time Step 27, Training loss for one mini batch: 3.3130\n",
      "Epoch 7, Time Step 28, Training loss for one mini batch: 1.9728\n",
      "Epoch 7, Time Step 29, Training loss for one mini batch: 2.7377\n",
      "Epoch 7, Time Step 30, Training loss for one mini batch: 1.8816\n",
      "Epoch 7, Time Step 31, Training loss for one mini batch: 2.0484\n",
      "Epoch 7, Time Step 32, Training loss for one mini batch: 1.7277\n",
      "Epoch 7, Time Step 33, Training loss for one mini batch: 1.6150\n",
      "Epoch 7, Time Step 34, Training loss for one mini batch: 2.2323\n",
      "Epoch 7, Time Step 35, Training loss for one mini batch: 1.5140\n",
      "Epoch 7, Time Step 36, Training loss for one mini batch: 2.4072\n",
      "Epoch 7, Time Step 37, Training loss for one mini batch: 1.1678\n",
      "Epoch 7, Time Step 38, Training loss for one mini batch: 2.9064\n",
      "Epoch 7, Time Step 39, Training loss for one mini batch: 1.4165\n",
      "Epoch 7, Time Step 40, Training loss for one mini batch: 3.4849\n",
      "Epoch 7, Time Step 41, Training loss for one mini batch: 2.2387\n",
      "Epoch 7, Time Step 42, Training loss for one mini batch: 2.6930\n",
      "Epoch 7, Time Step 43, Training loss for one mini batch: 1.6344\n",
      "Epoch 7, Time Step 44, Training loss for one mini batch: 1.7760\n",
      "Epoch 7, Time Step 45, Training loss for one mini batch: 1.8774\n",
      "Epoch 7, Time Step 46, Training loss for one mini batch: 1.5262\n",
      "Epoch 7, Time Step 47, Training loss for one mini batch: 1.9287\n",
      "Epoch 7, Time Step 48, Training loss for one mini batch: 1.0975\n",
      "Epoch 7, Time Step 49, Training loss for one mini batch: 2.5026\n",
      "Epoch 7, Time Step 50, Training loss for one mini batch: 0.9061\n",
      "Epoch 7, Time Step 51, Training loss for one mini batch: 4.4910\n",
      "Epoch 7, Time Step 52, Training loss for one mini batch: 2.4738\n",
      "Epoch 7, Time Step 53, Training loss for one mini batch: 2.6649\n",
      "Epoch 7, Time Step 54, Training loss for one mini batch: 1.3775\n",
      "Epoch 7, Time Step 55, Training loss for one mini batch: 2.2144\n",
      "Epoch 7, Time Step 56, Training loss for one mini batch: 1.7989\n",
      "Epoch 7, Time Step 57, Training loss for one mini batch: 1.9237\n",
      "Epoch 7, Time Step 58, Training loss for one mini batch: 2.1368\n",
      "Epoch 7, Time Step 59, Training loss for one mini batch: 1.7653\n",
      "Epoch 7, Time Step 60, Training loss for one mini batch: 2.8165\n",
      "Epoch 7, Time Step 61, Training loss for one mini batch: 1.2716\n",
      "Epoch 7, Time Step 62, Training loss for one mini batch: 2.4404\n",
      "Epoch 7, Time Step 63, Training loss for one mini batch: 0.9672\n",
      "Epoch 7, Time Step 64, Training loss for one mini batch: 4.4463\n",
      "Epoch 7, Time Step 65, Training loss for one mini batch: 2.7139\n",
      "Epoch 7, Time Step 66, Training loss for one mini batch: 3.3351\n",
      "Epoch 7, Time Step 67, Training loss for one mini batch: 1.9640\n",
      "Epoch 7, Time Step 68, Training loss for one mini batch: 2.1778\n",
      "Epoch 7, Time Step 69, Training loss for one mini batch: 1.8295\n",
      "Epoch 7, Time Step 70, Training loss for one mini batch: 2.0497\n",
      "Epoch 7, Time Step 71, Training loss for one mini batch: 2.6785\n",
      "Epoch 7, Time Step 72, Training loss for one mini batch: 1.7018\n",
      "Epoch 7, Time Step 73, Training loss for one mini batch: 2.3753\n",
      "Epoch 7, Time Step 74, Training loss for one mini batch: 1.4266\n",
      "Epoch 7, Time Step 75, Training loss for one mini batch: 3.0533\n",
      "Epoch 7, Time Step 76, Training loss for one mini batch: 1.5993\n",
      "Epoch 7, Time Step 77, Training loss for one mini batch: 4.1051\n",
      "Epoch 7, Time Step 78, Training loss for one mini batch: 2.8529\n",
      "Epoch 7, Time Step 79, Training loss for one mini batch: 3.1985\n",
      "Epoch 7, Time Step 80, Training loss for one mini batch: 2.2997\n",
      "Epoch 7, Time Step 81, Training loss for one mini batch: 2.4154\n",
      "Epoch 7, Time Step 82, Training loss for one mini batch: 2.4107\n",
      "Epoch 7, Time Step 83, Training loss for one mini batch: 1.9535\n",
      "Epoch 7, Time Step 84, Training loss for one mini batch: 2.1087\n",
      "Epoch 7, Time Step 85, Training loss for one mini batch: 1.5197\n",
      "Epoch 7, Time Step 86, Training loss for one mini batch: 2.4264\n",
      "Epoch 7, Time Step 87, Training loss for one mini batch: 1.4551\n",
      "Epoch 7, Time Step 88, Training loss for one mini batch: 3.1774\n",
      "Epoch 7, Time Step 89, Training loss for one mini batch: 2.1482\n",
      "Epoch 7, Time Step 90, Training loss for one mini batch: 3.5763\n",
      "Epoch 7, Time Step 91, Training loss for one mini batch: 2.6096\n",
      "Epoch 7, Time Step 92, Training loss for one mini batch: 2.7366\n",
      "Epoch 7, Time Step 93, Training loss for one mini batch: 2.2170\n",
      "Epoch 7, Time Step 94, Training loss for one mini batch: 2.4546\n",
      "Epoch 7, Time Step 95, Training loss for one mini batch: 2.5007\n",
      "Epoch 7, Time Step 96, Training loss for one mini batch: 2.0492\n",
      "Epoch 7, Time Step 97, Training loss for one mini batch: 2.4649\n",
      "Epoch 7, Time Step 98, Training loss for one mini batch: 1.7821\n",
      "Epoch 7, Time Step 99, Training loss for one mini batch: 2.8558\n",
      "Epoch 7, Time Step 100, Training loss for one mini batch: 2.0220\n",
      "Epoch 7, Training Accuracy: 0.00\n",
      "\n",
      "Epoch 7, Testing Loss for last mini batch: 3.2096\n",
      "Epoch 7, Testing Accuracy: 0.00\n",
      "\n",
      "\n",
      "\n",
      "Epoch 8, Time Step 1, Training loss for one mini batch: 3.2136\n",
      "Epoch 8, Time Step 2, Training loss for one mini batch: 2.2696\n",
      "Epoch 8, Time Step 3, Training loss for one mini batch: 2.8626\n",
      "Epoch 8, Time Step 4, Training loss for one mini batch: 2.0176\n",
      "Epoch 8, Time Step 5, Training loss for one mini batch: 1.9383\n",
      "Epoch 8, Time Step 6, Training loss for one mini batch: 1.4789\n",
      "Epoch 8, Time Step 7, Training loss for one mini batch: 1.9687\n",
      "Epoch 8, Time Step 8, Training loss for one mini batch: 2.4148\n",
      "Epoch 8, Time Step 9, Training loss for one mini batch: 1.9853\n",
      "Epoch 8, Time Step 10, Training loss for one mini batch: 2.6208\n",
      "Epoch 8, Time Step 11, Training loss for one mini batch: 1.7907\n",
      "Epoch 8, Time Step 12, Training loss for one mini batch: 2.8379\n",
      "Epoch 8, Time Step 13, Training loss for one mini batch: 1.5883\n",
      "Epoch 8, Time Step 14, Training loss for one mini batch: 3.5893\n",
      "Epoch 8, Time Step 15, Training loss for one mini batch: 1.9510\n",
      "Epoch 8, Time Step 16, Training loss for one mini batch: 2.9730\n",
      "Epoch 8, Time Step 17, Training loss for one mini batch: 1.9959\n",
      "Epoch 8, Time Step 18, Training loss for one mini batch: 2.2363\n",
      "Epoch 8, Time Step 19, Training loss for one mini batch: 1.7475\n",
      "Epoch 8, Time Step 20, Training loss for one mini batch: 1.7851\n",
      "Epoch 8, Time Step 21, Training loss for one mini batch: 2.3478\n",
      "Epoch 8, Time Step 22, Training loss for one mini batch: 1.7147\n",
      "Epoch 8, Time Step 23, Training loss for one mini batch: 2.1553\n",
      "Epoch 8, Time Step 24, Training loss for one mini batch: 1.1469\n",
      "Epoch 8, Time Step 25, Training loss for one mini batch: 2.7986\n",
      "Epoch 8, Time Step 26, Training loss for one mini batch: 1.3262\n",
      "Epoch 8, Time Step 27, Training loss for one mini batch: 3.2622\n",
      "Epoch 8, Time Step 28, Training loss for one mini batch: 1.9276\n",
      "Epoch 8, Time Step 29, Training loss for one mini batch: 2.7281\n",
      "Epoch 8, Time Step 30, Training loss for one mini batch: 1.8655\n",
      "Epoch 8, Time Step 31, Training loss for one mini batch: 2.0367\n",
      "Epoch 8, Time Step 32, Training loss for one mini batch: 1.7232\n",
      "Epoch 8, Time Step 33, Training loss for one mini batch: 1.5689\n",
      "Epoch 8, Time Step 34, Training loss for one mini batch: 2.1268\n",
      "Epoch 8, Time Step 35, Training loss for one mini batch: 1.4619\n",
      "Epoch 8, Time Step 36, Training loss for one mini batch: 2.3492\n",
      "Epoch 8, Time Step 37, Training loss for one mini batch: 1.1300\n",
      "Epoch 8, Time Step 38, Training loss for one mini batch: 2.7598\n",
      "Epoch 8, Time Step 39, Training loss for one mini batch: 1.3626\n",
      "Epoch 8, Time Step 40, Training loss for one mini batch: 3.2772\n",
      "Epoch 8, Time Step 41, Training loss for one mini batch: 2.0352\n",
      "Epoch 8, Time Step 42, Training loss for one mini batch: 2.5961\n",
      "Epoch 8, Time Step 43, Training loss for one mini batch: 1.5868\n",
      "Epoch 8, Time Step 44, Training loss for one mini batch: 1.7675\n",
      "Epoch 8, Time Step 45, Training loss for one mini batch: 1.8989\n",
      "Epoch 8, Time Step 46, Training loss for one mini batch: 1.5481\n",
      "Epoch 8, Time Step 47, Training loss for one mini batch: 1.9367\n",
      "Epoch 8, Time Step 48, Training loss for one mini batch: 1.0951\n",
      "Epoch 8, Time Step 49, Training loss for one mini batch: 2.4095\n",
      "Epoch 8, Time Step 50, Training loss for one mini batch: 0.8776\n",
      "Epoch 8, Time Step 51, Training loss for one mini batch: 4.4650\n",
      "Epoch 8, Time Step 52, Training loss for one mini batch: 2.4873\n",
      "Epoch 8, Time Step 53, Training loss for one mini batch: 2.6143\n",
      "Epoch 8, Time Step 54, Training loss for one mini batch: 1.2727\n",
      "Epoch 8, Time Step 55, Training loss for one mini batch: 2.1084\n",
      "Epoch 8, Time Step 56, Training loss for one mini batch: 1.7022\n",
      "Epoch 8, Time Step 57, Training loss for one mini batch: 1.8830\n",
      "Epoch 8, Time Step 58, Training loss for one mini batch: 2.1079\n",
      "Epoch 8, Time Step 59, Training loss for one mini batch: 1.7184\n",
      "Epoch 8, Time Step 60, Training loss for one mini batch: 2.7188\n",
      "Epoch 8, Time Step 61, Training loss for one mini batch: 1.2414\n",
      "Epoch 8, Time Step 62, Training loss for one mini batch: 2.4189\n",
      "Epoch 8, Time Step 63, Training loss for one mini batch: 0.9807\n",
      "Epoch 8, Time Step 64, Training loss for one mini batch: 4.3650\n",
      "Epoch 8, Time Step 65, Training loss for one mini batch: 2.6152\n",
      "Epoch 8, Time Step 66, Training loss for one mini batch: 3.2935\n",
      "Epoch 8, Time Step 67, Training loss for one mini batch: 1.9437\n",
      "Epoch 8, Time Step 68, Training loss for one mini batch: 2.1102\n",
      "Epoch 8, Time Step 69, Training loss for one mini batch: 1.7661\n",
      "Epoch 8, Time Step 70, Training loss for one mini batch: 2.0404\n",
      "Epoch 8, Time Step 71, Training loss for one mini batch: 2.6705\n",
      "Epoch 8, Time Step 72, Training loss for one mini batch: 1.6921\n",
      "Epoch 8, Time Step 73, Training loss for one mini batch: 2.3786\n",
      "Epoch 8, Time Step 74, Training loss for one mini batch: 1.4162\n",
      "Epoch 8, Time Step 75, Training loss for one mini batch: 3.0469\n",
      "Epoch 8, Time Step 76, Training loss for one mini batch: 1.5976\n",
      "Epoch 8, Time Step 77, Training loss for one mini batch: 4.0228\n",
      "Epoch 8, Time Step 78, Training loss for one mini batch: 2.7480\n",
      "Epoch 8, Time Step 79, Training loss for one mini batch: 3.1359\n",
      "Epoch 8, Time Step 80, Training loss for one mini batch: 2.2105\n",
      "Epoch 8, Time Step 81, Training loss for one mini batch: 2.3692\n",
      "Epoch 8, Time Step 82, Training loss for one mini batch: 2.3719\n",
      "Epoch 8, Time Step 83, Training loss for one mini batch: 1.9317\n",
      "Epoch 8, Time Step 84, Training loss for one mini batch: 2.1286\n",
      "Epoch 8, Time Step 85, Training loss for one mini batch: 1.5238\n",
      "Epoch 8, Time Step 86, Training loss for one mini batch: 2.4030\n",
      "Epoch 8, Time Step 87, Training loss for one mini batch: 1.4363\n",
      "Epoch 8, Time Step 88, Training loss for one mini batch: 3.1031\n",
      "Epoch 8, Time Step 89, Training loss for one mini batch: 2.0835\n",
      "Epoch 8, Time Step 90, Training loss for one mini batch: 3.5092\n",
      "Epoch 8, Time Step 91, Training loss for one mini batch: 2.5333\n",
      "Epoch 8, Time Step 92, Training loss for one mini batch: 2.6163\n",
      "Epoch 8, Time Step 93, Training loss for one mini batch: 2.0904\n",
      "Epoch 8, Time Step 94, Training loss for one mini batch: 2.3714\n",
      "Epoch 8, Time Step 95, Training loss for one mini batch: 2.4366\n",
      "Epoch 8, Time Step 96, Training loss for one mini batch: 2.0250\n",
      "Epoch 8, Time Step 97, Training loss for one mini batch: 2.4359\n",
      "Epoch 8, Time Step 98, Training loss for one mini batch: 1.7510\n",
      "Epoch 8, Time Step 99, Training loss for one mini batch: 2.7782\n",
      "Epoch 8, Time Step 100, Training loss for one mini batch: 1.9648\n",
      "Epoch 8, Training Accuracy: 0.00\n",
      "\n",
      "Epoch 8, Testing Loss for last mini batch: 3.2096\n",
      "Epoch 8, Testing Accuracy: 0.00\n",
      "\n",
      "\n",
      "\n",
      "Epoch 9, Time Step 1, Training loss for one mini batch: 3.2022\n",
      "Epoch 9, Time Step 2, Training loss for one mini batch: 2.2805\n",
      "Epoch 9, Time Step 3, Training loss for one mini batch: 2.8312\n",
      "Epoch 9, Time Step 4, Training loss for one mini batch: 1.9840\n",
      "Epoch 9, Time Step 5, Training loss for one mini batch: 1.8915\n",
      "Epoch 9, Time Step 6, Training loss for one mini batch: 1.4371\n",
      "Epoch 9, Time Step 7, Training loss for one mini batch: 1.9400\n",
      "Epoch 9, Time Step 8, Training loss for one mini batch: 2.3818\n",
      "Epoch 9, Time Step 9, Training loss for one mini batch: 1.9634\n",
      "Epoch 9, Time Step 10, Training loss for one mini batch: 2.5986\n",
      "Epoch 9, Time Step 11, Training loss for one mini batch: 1.7751\n",
      "Epoch 9, Time Step 12, Training loss for one mini batch: 2.7756\n",
      "Epoch 9, Time Step 13, Training loss for one mini batch: 1.5327\n",
      "Epoch 9, Time Step 14, Training loss for one mini batch: 3.5911\n",
      "Epoch 9, Time Step 15, Training loss for one mini batch: 1.9277\n",
      "Epoch 9, Time Step 16, Training loss for one mini batch: 2.8828\n",
      "Epoch 9, Time Step 17, Training loss for one mini batch: 1.9584\n",
      "Epoch 9, Time Step 18, Training loss for one mini batch: 2.2347\n",
      "Epoch 9, Time Step 19, Training loss for one mini batch: 1.7449\n",
      "Epoch 9, Time Step 20, Training loss for one mini batch: 1.7260\n",
      "Epoch 9, Time Step 21, Training loss for one mini batch: 2.2328\n",
      "Epoch 9, Time Step 22, Training loss for one mini batch: 1.6487\n",
      "Epoch 9, Time Step 23, Training loss for one mini batch: 2.0992\n",
      "Epoch 9, Time Step 24, Training loss for one mini batch: 1.1242\n",
      "Epoch 9, Time Step 25, Training loss for one mini batch: 2.7422\n",
      "Epoch 9, Time Step 26, Training loss for one mini batch: 1.3154\n",
      "Epoch 9, Time Step 27, Training loss for one mini batch: 3.2727\n",
      "Epoch 9, Time Step 28, Training loss for one mini batch: 1.9451\n",
      "Epoch 9, Time Step 29, Training loss for one mini batch: 2.7193\n",
      "Epoch 9, Time Step 30, Training loss for one mini batch: 1.8414\n",
      "Epoch 9, Time Step 31, Training loss for one mini batch: 1.9741\n",
      "Epoch 9, Time Step 32, Training loss for one mini batch: 1.6516\n",
      "Epoch 9, Time Step 33, Training loss for one mini batch: 1.5409\n",
      "Epoch 9, Time Step 34, Training loss for one mini batch: 2.1070\n",
      "Epoch 9, Time Step 35, Training loss for one mini batch: 1.4449\n",
      "Epoch 9, Time Step 36, Training loss for one mini batch: 2.2721\n",
      "Epoch 9, Time Step 37, Training loss for one mini batch: 1.0699\n",
      "Epoch 9, Time Step 38, Training loss for one mini batch: 2.7011\n",
      "Epoch 9, Time Step 39, Training loss for one mini batch: 1.3439\n",
      "Epoch 9, Time Step 40, Training loss for one mini batch: 3.1061\n",
      "Epoch 9, Time Step 41, Training loss for one mini batch: 1.8671\n",
      "Epoch 9, Time Step 42, Training loss for one mini batch: 2.5006\n",
      "Epoch 9, Time Step 43, Training loss for one mini batch: 1.5445\n",
      "Epoch 9, Time Step 44, Training loss for one mini batch: 1.7260\n",
      "Epoch 9, Time Step 45, Training loss for one mini batch: 1.8765\n",
      "Epoch 9, Time Step 46, Training loss for one mini batch: 1.5798\n",
      "Epoch 9, Time Step 47, Training loss for one mini batch: 1.9879\n",
      "Epoch 9, Time Step 48, Training loss for one mini batch: 1.1142\n",
      "Epoch 9, Time Step 49, Training loss for one mini batch: 2.3680\n",
      "Epoch 9, Time Step 50, Training loss for one mini batch: 0.8681\n",
      "Epoch 9, Time Step 51, Training loss for one mini batch: 4.4361\n",
      "Epoch 9, Time Step 52, Training loss for one mini batch: 2.5003\n",
      "Epoch 9, Time Step 53, Training loss for one mini batch: 2.5812\n",
      "Epoch 9, Time Step 54, Training loss for one mini batch: 1.1975\n",
      "Epoch 9, Time Step 55, Training loss for one mini batch: 2.0297\n",
      "Epoch 9, Time Step 56, Training loss for one mini batch: 1.6403\n",
      "Epoch 9, Time Step 57, Training loss for one mini batch: 1.8929\n",
      "Epoch 9, Time Step 58, Training loss for one mini batch: 2.1324\n",
      "Epoch 9, Time Step 59, Training loss for one mini batch: 1.6966\n",
      "Epoch 9, Time Step 60, Training loss for one mini batch: 2.6030\n",
      "Epoch 9, Time Step 61, Training loss for one mini batch: 1.2131\n",
      "Epoch 9, Time Step 62, Training loss for one mini batch: 2.4443\n",
      "Epoch 9, Time Step 63, Training loss for one mini batch: 1.0098\n",
      "Epoch 9, Time Step 64, Training loss for one mini batch: 4.1951\n",
      "Epoch 9, Time Step 65, Training loss for one mini batch: 2.4406\n",
      "Epoch 9, Time Step 66, Training loss for one mini batch: 3.2247\n",
      "Epoch 9, Time Step 67, Training loss for one mini batch: 1.9369\n",
      "Epoch 9, Time Step 68, Training loss for one mini batch: 2.0818\n",
      "Epoch 9, Time Step 69, Training loss for one mini batch: 1.7364\n",
      "Epoch 9, Time Step 70, Training loss for one mini batch: 2.0163\n",
      "Epoch 9, Time Step 71, Training loss for one mini batch: 2.6458\n",
      "Epoch 9, Time Step 72, Training loss for one mini batch: 1.6596\n",
      "Epoch 9, Time Step 73, Training loss for one mini batch: 2.3277\n",
      "Epoch 9, Time Step 74, Training loss for one mini batch: 1.3821\n",
      "Epoch 9, Time Step 75, Training loss for one mini batch: 3.0088\n",
      "Epoch 9, Time Step 76, Training loss for one mini batch: 1.5864\n",
      "Epoch 9, Time Step 77, Training loss for one mini batch: 3.9731\n",
      "Epoch 9, Time Step 78, Training loss for one mini batch: 2.7025\n",
      "Epoch 9, Time Step 79, Training loss for one mini batch: 3.1097\n",
      "Epoch 9, Time Step 80, Training loss for one mini batch: 2.1596\n",
      "Epoch 9, Time Step 81, Training loss for one mini batch: 2.3111\n",
      "Epoch 9, Time Step 82, Training loss for one mini batch: 2.3206\n",
      "Epoch 9, Time Step 83, Training loss for one mini batch: 1.9118\n",
      "Epoch 9, Time Step 84, Training loss for one mini batch: 2.1505\n",
      "Epoch 9, Time Step 85, Training loss for one mini batch: 1.5420\n",
      "Epoch 9, Time Step 86, Training loss for one mini batch: 2.3786\n",
      "Epoch 9, Time Step 87, Training loss for one mini batch: 1.4722\n",
      "Epoch 9, Time Step 88, Training loss for one mini batch: 3.0605\n",
      "Epoch 9, Time Step 89, Training loss for one mini batch: 2.0319\n",
      "Epoch 9, Time Step 90, Training loss for one mini batch: 3.3998\n",
      "Epoch 9, Time Step 91, Training loss for one mini batch: 2.4206\n",
      "Epoch 9, Time Step 92, Training loss for one mini batch: 2.4954\n",
      "Epoch 9, Time Step 93, Training loss for one mini batch: 1.9732\n",
      "Epoch 9, Time Step 94, Training loss for one mini batch: 2.2937\n",
      "Epoch 9, Time Step 95, Training loss for one mini batch: 2.3597\n",
      "Epoch 9, Time Step 96, Training loss for one mini batch: 1.9870\n",
      "Epoch 9, Time Step 97, Training loss for one mini batch: 2.4025\n",
      "Epoch 9, Time Step 98, Training loss for one mini batch: 1.7213\n",
      "Epoch 9, Time Step 99, Training loss for one mini batch: 2.7088\n",
      "Epoch 9, Time Step 100, Training loss for one mini batch: 1.9092\n",
      "Epoch 9, Training Accuracy: 0.00\n",
      "\n",
      "Epoch 9, Testing Loss for last mini batch: 3.2096\n",
      "Epoch 9, Testing Accuracy: 0.00\n",
      "\n",
      "\n",
      "\n",
      "Epoch 10, Time Step 1, Training loss for one mini batch: 3.2094\n",
      "Epoch 10, Time Step 2, Training loss for one mini batch: 2.3041\n",
      "Epoch 10, Time Step 3, Training loss for one mini batch: 2.7921\n",
      "Epoch 10, Time Step 4, Training loss for one mini batch: 1.9468\n",
      "Epoch 10, Time Step 5, Training loss for one mini batch: 1.8484\n",
      "Epoch 10, Time Step 6, Training loss for one mini batch: 1.4094\n",
      "Epoch 10, Time Step 7, Training loss for one mini batch: 1.9264\n",
      "Epoch 10, Time Step 8, Training loss for one mini batch: 2.3732\n",
      "Epoch 10, Time Step 9, Training loss for one mini batch: 1.9470\n",
      "Epoch 10, Time Step 10, Training loss for one mini batch: 2.5313\n",
      "Epoch 10, Time Step 11, Training loss for one mini batch: 1.7088\n",
      "Epoch 10, Time Step 12, Training loss for one mini batch: 2.6890\n",
      "Epoch 10, Time Step 13, Training loss for one mini batch: 1.4841\n",
      "Epoch 10, Time Step 14, Training loss for one mini batch: 3.5993\n",
      "Epoch 10, Time Step 15, Training loss for one mini batch: 1.9272\n",
      "Epoch 10, Time Step 16, Training loss for one mini batch: 2.8525\n",
      "Epoch 10, Time Step 17, Training loss for one mini batch: 1.9679\n",
      "Epoch 10, Time Step 18, Training loss for one mini batch: 2.2320\n",
      "Epoch 10, Time Step 19, Training loss for one mini batch: 1.7129\n",
      "Epoch 10, Time Step 20, Training loss for one mini batch: 1.6685\n",
      "Epoch 10, Time Step 21, Training loss for one mini batch: 2.1323\n",
      "Epoch 10, Time Step 22, Training loss for one mini batch: 1.5882\n",
      "Epoch 10, Time Step 23, Training loss for one mini batch: 2.0941\n",
      "Epoch 10, Time Step 24, Training loss for one mini batch: 1.1203\n",
      "Epoch 10, Time Step 25, Training loss for one mini batch: 2.6539\n",
      "Epoch 10, Time Step 26, Training loss for one mini batch: 1.2729\n",
      "Epoch 10, Time Step 27, Training loss for one mini batch: 3.2308\n",
      "Epoch 10, Time Step 28, Training loss for one mini batch: 1.9081\n",
      "Epoch 10, Time Step 29, Training loss for one mini batch: 2.6880\n",
      "Epoch 10, Time Step 30, Training loss for one mini batch: 1.8444\n",
      "Epoch 10, Time Step 31, Training loss for one mini batch: 1.9772\n",
      "Epoch 10, Time Step 32, Training loss for one mini batch: 1.6512\n",
      "Epoch 10, Time Step 33, Training loss for one mini batch: 1.5307\n",
      "Epoch 10, Time Step 34, Training loss for one mini batch: 2.0880\n",
      "Epoch 10, Time Step 35, Training loss for one mini batch: 1.4267\n",
      "Epoch 10, Time Step 36, Training loss for one mini batch: 2.2071\n",
      "Epoch 10, Time Step 37, Training loss for one mini batch: 1.0120\n",
      "Epoch 10, Time Step 38, Training loss for one mini batch: 2.6710\n",
      "Epoch 10, Time Step 39, Training loss for one mini batch: 1.3276\n",
      "Epoch 10, Time Step 40, Training loss for one mini batch: 3.0049\n",
      "Epoch 10, Time Step 41, Training loss for one mini batch: 1.7750\n",
      "Epoch 10, Time Step 42, Training loss for one mini batch: 2.4225\n",
      "Epoch 10, Time Step 43, Training loss for one mini batch: 1.4852\n",
      "Epoch 10, Time Step 44, Training loss for one mini batch: 1.6910\n",
      "Epoch 10, Time Step 45, Training loss for one mini batch: 1.8707\n",
      "Epoch 10, Time Step 46, Training loss for one mini batch: 1.6270\n",
      "Epoch 10, Time Step 47, Training loss for one mini batch: 2.0183\n",
      "Epoch 10, Time Step 48, Training loss for one mini batch: 1.1281\n",
      "Epoch 10, Time Step 49, Training loss for one mini batch: 2.3287\n",
      "Epoch 10, Time Step 50, Training loss for one mini batch: 0.8678\n",
      "Epoch 10, Time Step 51, Training loss for one mini batch: 4.3992\n",
      "Epoch 10, Time Step 52, Training loss for one mini batch: 2.4739\n",
      "Epoch 10, Time Step 53, Training loss for one mini batch: 2.5000\n",
      "Epoch 10, Time Step 54, Training loss for one mini batch: 1.1134\n",
      "Epoch 10, Time Step 55, Training loss for one mini batch: 1.9687\n",
      "Epoch 10, Time Step 56, Training loss for one mini batch: 1.6028\n",
      "Epoch 10, Time Step 57, Training loss for one mini batch: 1.8863\n",
      "Epoch 10, Time Step 58, Training loss for one mini batch: 2.1327\n",
      "Epoch 10, Time Step 59, Training loss for one mini batch: 1.6733\n",
      "Epoch 10, Time Step 60, Training loss for one mini batch: 2.4987\n",
      "Epoch 10, Time Step 61, Training loss for one mini batch: 1.1830\n",
      "Epoch 10, Time Step 62, Training loss for one mini batch: 2.4602\n",
      "Epoch 10, Time Step 63, Training loss for one mini batch: 1.0039\n",
      "Epoch 10, Time Step 64, Training loss for one mini batch: 4.0545\n",
      "Epoch 10, Time Step 65, Training loss for one mini batch: 2.2925\n",
      "Epoch 10, Time Step 66, Training loss for one mini batch: 3.1044\n",
      "Epoch 10, Time Step 67, Training loss for one mini batch: 1.8568\n",
      "Epoch 10, Time Step 68, Training loss for one mini batch: 2.0212\n",
      "Epoch 10, Time Step 69, Training loss for one mini batch: 1.6978\n",
      "Epoch 10, Time Step 70, Training loss for one mini batch: 1.9926\n",
      "Epoch 10, Time Step 71, Training loss for one mini batch: 2.6020\n",
      "Epoch 10, Time Step 72, Training loss for one mini batch: 1.6451\n",
      "Epoch 10, Time Step 73, Training loss for one mini batch: 2.3086\n",
      "Epoch 10, Time Step 74, Training loss for one mini batch: 1.3687\n",
      "Epoch 10, Time Step 75, Training loss for one mini batch: 2.9607\n",
      "Epoch 10, Time Step 76, Training loss for one mini batch: 1.5568\n",
      "Epoch 10, Time Step 77, Training loss for one mini batch: 3.9399\n",
      "Epoch 10, Time Step 78, Training loss for one mini batch: 2.6253\n",
      "Epoch 10, Time Step 79, Training loss for one mini batch: 3.0573\n",
      "Epoch 10, Time Step 80, Training loss for one mini batch: 2.1049\n",
      "Epoch 10, Time Step 81, Training loss for one mini batch: 2.3294\n",
      "Epoch 10, Time Step 82, Training loss for one mini batch: 2.3343\n",
      "Epoch 10, Time Step 83, Training loss for one mini batch: 1.8765\n",
      "Epoch 10, Time Step 84, Training loss for one mini batch: 2.1036\n",
      "Epoch 10, Time Step 85, Training loss for one mini batch: 1.5319\n",
      "Epoch 10, Time Step 86, Training loss for one mini batch: 2.4228\n",
      "Epoch 10, Time Step 87, Training loss for one mini batch: 1.5026\n",
      "Epoch 10, Time Step 88, Training loss for one mini batch: 3.0227\n",
      "Epoch 10, Time Step 89, Training loss for one mini batch: 1.9634\n",
      "Epoch 10, Time Step 90, Training loss for one mini batch: 3.2869\n",
      "Epoch 10, Time Step 91, Training loss for one mini batch: 2.3040\n",
      "Epoch 10, Time Step 92, Training loss for one mini batch: 2.3734\n",
      "Epoch 10, Time Step 93, Training loss for one mini batch: 1.8557\n",
      "Epoch 10, Time Step 94, Training loss for one mini batch: 2.2284\n",
      "Epoch 10, Time Step 95, Training loss for one mini batch: 2.3201\n",
      "Epoch 10, Time Step 96, Training loss for one mini batch: 1.9710\n",
      "Epoch 10, Time Step 97, Training loss for one mini batch: 2.3290\n",
      "Epoch 10, Time Step 98, Training loss for one mini batch: 1.6669\n",
      "Epoch 10, Time Step 99, Training loss for one mini batch: 2.6434\n",
      "Epoch 10, Time Step 100, Training loss for one mini batch: 1.8590\n",
      "Epoch 10, Training Accuracy: 0.00\n",
      "\n",
      "Epoch 10, Testing Loss for last mini batch: 3.2096\n",
      "Epoch 10, Testing Accuracy: 0.00\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    training_data_generator = custom_data_generator(training_data,Y_true_train,782)\n",
    "\n",
    "    for time_step, (X_train_mb, Y_true_train_mb) in enumerate(training_data_generator):\n",
    "        training_loss = training_step(X_train_mb,Y_true_train_mb)\n",
    "\n",
    "        #if (time_step+1) % 50 == 0:\n",
    "        print(\"Epoch %d, Time Step %d, Training loss for one mini batch: %.4f\"\n",
    "            % (epoch+1, time_step+1, float(training_loss)))\n",
    "            \n",
    "    training_acc = train_acc_metric.result()    \n",
    "    print(\"Epoch %d, Training Accuracy: %.2f\" % (epoch+1,float(training_acc)))\n",
    "    train_acc_metric.reset_state()\n",
    "\n",
    "    for X_test_mb, Y_true_test_mb in testing_data_generator:\n",
    "        testing_loss = testing_forward_pass(X_test_mb,Y_true_test_mb)\n",
    "\n",
    "    print(\"\\nEpoch %d, Testing Loss for last mini batch: %.4f\" % (epoch+1,float(testing_loss)))\n",
    "    testing_acc = test_acc_metric.result()\n",
    "    print(\"Epoch %d, Testing Accuracy: %.2f\" % (epoch+1,float(testing_acc)))\n",
    "    test_acc_metric.reset_state()\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
